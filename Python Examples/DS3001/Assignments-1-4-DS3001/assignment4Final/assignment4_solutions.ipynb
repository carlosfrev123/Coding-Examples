{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac",
   "metadata": {},
   "source": [
    "# Assignment #4: Linear Models and Decision Trees\n",
    "## Foundations of Machine Learning\n",
    "## Do Q1 and one other question.\n",
    "### Advice: Reuse your code and code from lectures, package routine tasks into functions, make plans about how you'll carry out the analysis before jumping into writing code, and work as efficiently as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc6d72-98b8-4642-9390-29109087101a",
   "metadata": {},
   "source": [
    "**Q1.** This question is a case study for linear models and decision trees. The data are about car prices. In particular, they include:\n",
    "\n",
    "  - `Price`, `Color`, `Seating_Capacity`\n",
    "  - `Body_Type`: crossover, hatchback, muv, sedan, suv \n",
    "  - `Make`, `Make_Year`: The brand of car and year produced\n",
    "  - `Mileage_Run`: The number of miles on the odometer\n",
    "  - `Fuel_Type`: Diesel or gasoline/petrol\n",
    "  - `Transmission`, `Transmission_Type`:  speeds and automatic/manual\n",
    "\n",
    "  1. Load `cars_hw.csv`. These data were really dirty, and I've already cleaned them a significant amount in terms of missing values and other issues, but some issues remain (e.g. outliers, badly scaled variables that require a log or arcsinh transformation). Clean the data however you think is most appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c878b-dbcc-46c2-8d2d-96a6df14444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./data/cars_hw.csv')\n",
    "df0 = df # Save the original data for a few plots\n",
    "sns.boxplot(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7cf126-4815-4c5c-a3ff-a2e4f2029e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_ihs'] = np.arcsinh(df['Price'])\n",
    "df['mileage_ihs'] = np.arcsinh(df['Mileage_Run'])\n",
    "df['age'] = max(df['Make_Year'])-df['Make_Year']\n",
    "df = df.drop(['Price','Mileage_Run','Make_Year','Unnamed: 0'],axis=1)\n",
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99e06f-7f46-4658-b748-b0738287fbc1",
   "metadata": {},
   "source": [
    "  2. Summarize the `Price` variable and create a kernel density plot. Use `.groupby()` and `.describe()` to summarize prices by brand (`Make`). Make a grouped kernel density plot by `Make`. Which car brands are the most expensive? What do prices look like in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fae4bb-5a1a-4c75-81f6-929fdb74c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data = df0, x='Price',hue='Make')\n",
    "df0['Price'].groupby(df0['Make']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a692f-af9e-4b1a-a4fa-9676117c1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df,x='price_ihs',hue='Make')\n",
    "df['price_ihs'].groupby(df['Make']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f2db6-db5a-4b0b-95e6-f389c969db5b",
   "metadata": {},
   "source": [
    "> The MG Motors cars are by far the most expensive, then Kia, then Jeep. The kernel density plot shows that car prices are typically single peaked, except for Maruti Suzuki and Toyota. I thought there would be more of this multi-peakedness, where each peak corresponded to a particular quality/price point in the market (e.g. an entry level sedan, a luxury sedan, an SUV, and a sportscar). Prices range from 188,000 for a cheap Tata to 2,941,000 for an expensive Skoda. These are Indian Rupees, so the Skoda would cost 35,281.77 dollars and the Tata would cost 2,255.35 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff794c-db75-4be9-8d1d-d76bf3ed95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df.loc[ df['Make'].isin(['Maruti Suzuki','Toyota'])  ,:],x='price_ihs',hue='Make')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad86489-035f-4689-bc49-80dde66ecbc3",
   "metadata": {},
   "source": [
    "  3. Split the data into an 80% training set and a 20% testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a9175-cca7-47d8-b7b1-9174f58175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the rows in the dataframe:\n",
    "N = df.shape[0]\n",
    "df = df.sample(frac=1, random_state=100) # randomize the order in which data appears\n",
    "train_size = int(.8*N)\n",
    "\n",
    "# How to do the split as needed:\n",
    "df_train = df[0:train_size]\n",
    "y_train = df_train['price_ihs']\n",
    "\n",
    "df_test = df[train_size:]\n",
    "y_test = df_test['price_ihs']\n",
    "\n",
    "# So the first 0:train_size rows are my training data, and train_size: are my test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de78005-80e4-4365-b290-83f36263a940",
   "metadata": {},
   "source": [
    "  4. Let's focus on linear models. Make a model where you regress price on the numeric variables alone; what is the $R^2$ and `RMSE` on the test set? Make a second model where, for the categorical variables, make a model comprised of one-hot encoded regressors/features alone, and regress price on those variables; what is the $R^2$ and `RMSE` on the test set? Which model performs better on the test set? Make a third model that combines all the regressors from the previous two; what is the $R^2$ and `RMSE` on the test set? Does the joint model perform better or worse, and by home much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20ba30-db07-437e-9e95-840d8028e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Numeric regression:\n",
    "var_n = ['age','mileage_ihs','Seating_Capacity'] # Select variables\n",
    "X_train_n = df_train.loc[:,var_n] # Process training covariates\n",
    "reg_n = linear_model.LinearRegression().fit(X_train_n,y_train) # Run regression\n",
    "X_test_n = df_test.loc[:,var_n] # Process test covariates\n",
    "y_hat = reg_n.predict(X_test_n)\n",
    "print('Numeric only Rsq: ', reg_n.score(X_test_n,y_test)) # R2\n",
    "rmse_n = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "print('Numeric only RMSE: ', rmse_n) # R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ec6e4-abaf-4bb2-9021-4f7ea96f7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical regression:\n",
    "var_cat = ['Make','Body_Type','Color','Fuel_Type','Transmission','Transmission_Type']\n",
    "#\n",
    "# Use a loop to make the dummy dataframe, but you could do it by hand in a bunch of lines:\n",
    "dummies = pd.DataFrame([]) # In general, use lists and append instead of DataFrame and pd.concat\n",
    "for var in var_cat: \n",
    "    new_dummies = pd.get_dummies( df.loc[:,var], drop_first=True, dtype=int)\n",
    "    dummies = pd.concat([dummies, new_dummies], axis=1, ignore_index=True) \n",
    "    X_train_c = dummies.iloc[0:train_size,:]\n",
    "    X_test_c = dummies.iloc[train_size:,:]\n",
    "#\n",
    "reg_c = linear_model.LinearRegression().fit(X_train_c,y_train) # Run regression\n",
    "y_hat_c = reg_c.predict(X_test_c)\n",
    "print('Categorical only Rsq: ', reg_c.score(X_test_c,y_test)) # R2\n",
    "rmse_c = np.sqrt( np.mean( (y_test - y_hat_c)**2 ))\n",
    "print('Categorical only RMSE: ', rmse_c) # R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaad8b9-c108-4b1a-93df-bc8411dee2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numeric _n and categorical _c variables:\n",
    "X_train_all = pd.concat([X_train_n, X_train_c],axis=1, ignore_index=True)\n",
    "X_test_all = pd.concat([X_test_n, X_test_c],axis=1, ignore_index=True)\n",
    "#\n",
    "reg_all = linear_model.LinearRegression().fit(X_train_all,y_train) # Run regression\n",
    "y_hat_all = reg_all.predict(X_test_all)\n",
    "print('All Rsq: ', reg_all.score(X_test_all,y_test)) # R2\n",
    "rmse_all = np.sqrt( np.mean( (y_test - y_hat_all)**2 ))\n",
    "print('All RMSE: ', rmse_all) # R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940fb4f-6237-4155-b9ca-4915440ad017",
   "metadata": {},
   "source": [
    "> The joint model with numeric and categorical variables performs the best, achieving an Rsq of .716 and RMSE of .21. The numeric only achieved .328/.317 and the categorical only achieved .493/.275. So in this case, expanding the set of features improved the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00c39f-6baf-4125-a424-525ece4eae19",
   "metadata": {},
   "source": [
    "  5. Use the `PolynomialFeatures` function from `sklearn` to expand the set of numerical variables you're using, along with the categorical variables. As you increase the degree of the expansion, how do the $R^2$ and `RMSE` change? At what point does $R^2$ go negative on the test set? For your best model with expanded features, what is the $R^2$ and `RMSE`? How does it compare to your best model from part 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30f497-40f7-4923-a721-22a2273b3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "for d in np.arange(1,5):\n",
    "    expander = PolynomialFeatures(degree=d,include_bias=False)\n",
    "    #\n",
    "    # Expand variables:\n",
    "    Z = expander.fit_transform(X_test_n)\n",
    "    names = expander.get_feature_names_out() # Get the names of these variables\n",
    "    X_test_d = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "    Z = expander.fit_transform(X_train_n)\n",
    "    names = expander.get_feature_names_out() # Get the names of these variables\n",
    "    X_train_d = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "    X_train_d.reset_index(drop=True, inplace=True)\n",
    "    X_train_c.reset_index(drop=True, inplace=True)\n",
    "    X_train_all = pd.concat([X_train_d, X_train_c], axis=1, ignore_index=True)\n",
    "    #\n",
    "    # Reset indices for concatenating:\n",
    "    X_test_d.reset_index(drop=True, inplace=True)\n",
    "    X_test_c.reset_index(drop=True, inplace=True)\n",
    "    X_test_all = pd.concat([X_test_d, X_test_c], axis=1, ignore_index=True)\n",
    "    #\n",
    "    # Regression metrics:\n",
    "    reg_all = linear_model.LinearRegression().fit(X_train_all,y_train) # Run regression\n",
    "    print(d, ' Rsq: ', reg_all.score(X_test_all,y_test)) # R2\n",
    "    y_hat_all = reg_all.predict(X_test_all)\n",
    "    rmse_all = np.sqrt( np.mean( (y_test - y_hat_all)**2 ))\n",
    "    print(d, 'RMSE: ', rmse_all) # R2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae8963-b059-419e-9b4c-f3256d5eb35a",
   "metadata": {},
   "source": [
    "> So the best degree is 2: An Rsq of .74 and a RMSE of .196. After that, performance falls, with a negative Rsq for degree=4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53408caa-3f5d-4c7c-b2e6-ca4fdb3cd85a",
   "metadata": {},
   "source": [
    "  6. For your best model so far, determine the predicted values for the test data and plot them against the true values. Do the predicted values and true values roughly line up along the diagonal, or not? Compute the residuals/errors for the test data and create a kernel density plot. Do the residuals look roughly bell-shaped around zero? Evaluate the strengths and weaknesses of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6455957-cf20-4daf-b584-1b29e40e0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967ce24-c3bb-4fd2-bd9d-788865807837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse earlier code to get predictions:\n",
    "d = 2\n",
    "#\n",
    "expander = PolynomialFeatures(degree=d,include_bias=False)\n",
    "Z = expander.fit_transform(X_test_n)\n",
    "names = expander.get_feature_names_out() # Get the names of these variables\n",
    "X_test_d = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "#\n",
    "Z = expander.fit_transform(X_train_n)\n",
    "names = expander.get_feature_names_out() # Get the names of these variables\n",
    "X_train_d = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "#\n",
    "X_train_d.reset_index(drop=True, inplace=True)\n",
    "X_train_c.reset_index(drop=True, inplace=True)\n",
    "X_train_all = pd.concat([X_train_d, X_train_c], axis=1)\n",
    "X_train_all.columns = X_train_all.columns.astype(str)\n",
    "# Reset indices for concatenating:\n",
    "X_test_d.reset_index(drop=True, inplace=True)\n",
    "X_test_c.reset_index(drop=True, inplace=True)\n",
    "X_test_all = pd.concat([X_test_d, X_test_c], axis=1)\n",
    "X_test_all.columns = X_test_all.columns.astype(str)\n",
    "# Run regression   \n",
    "reg_all = linear_model.LinearRegression().fit(X_train_all,y_train) # Run regression\n",
    "# Predictions and residuals:\n",
    "y_hat_all = reg_all.predict(X_test_all)\n",
    "residuals = y_test - y_hat_all\n",
    "\n",
    "# Scatterplot:\n",
    "sns.scatterplot(x=y_test,y=y_hat_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cadefc-6f37-452b-9b9f-4b331e15f330",
   "metadata": {},
   "source": [
    "> Looks like the values line up pretty well along the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7f88f-1324-4a0e-bd1d-d43d026ff7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7526f-ccf1-40e5-8862-64f001a0baa4",
   "metadata": {},
   "source": [
    "> Looks slightly bi-modal with a bit of a left tail, but otherwise symmetric around zero and bell-shaped. Those two peaks might be a statistical artifact that would vanish if we did the test-train split differently, or it might be a real feature of the data. The bi-modal nature suggests there's more discrete, unobserved heterogeneity in the data that could explain why there are multiple peaks that we're just missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebf115-20a2-4271-a58d-2c441e70bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "\n",
    "X_train_n.reset_index(drop=True, inplace=True)\n",
    "X_train_c.reset_index(drop=True, inplace=True)\n",
    "X_train_all = pd.concat([X_train_n, X_train_c],axis=1)\n",
    "X_train_all.columns = X_train_all.columns.astype(str)\n",
    "X_test_n.reset_index(drop=True, inplace=True)\n",
    "X_test_c.reset_index(drop=True, inplace=True)\n",
    "X_test_all = pd.concat([X_test_n, X_test_c],axis=1)\n",
    "X_test_all.columns = X_test_all.columns.astype(str)\n",
    "\n",
    "sup_depth = 20\n",
    "for d in np.arange(2,sup_depth):\n",
    "    model = tree.DecisionTreeRegressor(max_depth=d) # Fit the classifier\n",
    "    cart = model.fit(X_train_all, y_train) # \n",
    "    cart.score(X_test_all,y_test)\n",
    "    y_hat = cart.predict(X_test_all)\n",
    "    rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "    print('Depth: ', d, ', RMSE: ', rmse, ', Rsq: ', cart.score(X_test_all,y_test)) # R2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae91a0d-9458-436f-afb5-2a04c285c3e9",
   "metadata": {},
   "source": [
    "  7. Now, let's use a regression tree. Construct an appropriate matrix of regressors/features, and fit a tree to the data. Vary the maximum depth of the decision tree using the `max_depth` option (i.e. `tree.DecisionTreeRegressor(max_depth=D)`), and compute the $R^2$ and `RMSE` on the test set of a variety of depths. What depth tree gives the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5a04d-32b7-408e-bb17-dbaa3eb88b21",
   "metadata": {},
   "source": [
    "> Looks like a depth of around 9 gives the best results, with an Rsq of .61 and RMSE of .24. (It's nice to do this train/test split to pick the depth, but we'll want to cross validate later instead.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fa058-277f-4414-bd32-8bc6e8126f86",
   "metadata": {},
   "source": [
    "  8. For your best tree, determine the predicted values for the test data, and plot them against the true values. Do the predicted values and true values line up along the diagonal, or not? Compute the residuals/errors for the test data and create a kernel density plot. Do the residuals look roughly bell-shaped around zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa5ce7-b1af-42ab-930e-8ed2f23163f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeRegressor(max_depth=9, random_state=100) \n",
    "cart = model.fit(X_train_all, y_train)\n",
    "y_hat = cart.predict(X_test_all)\n",
    "residuals = y_test - y_hat\n",
    "\n",
    "sns.scatterplot(x=y_test,y=y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a035b-edd9-4015-805b-e60cd117ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1441e0-c54a-4f5f-a2c4-fe72335c9469",
   "metadata": {},
   "source": [
    "> The residuals are roughly bell-shaped around zero, but have a bloop around .4. Again, it looks like perhaps there is some information we don't have or aren't using that could help us build a better model that fits the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1d776-07fc-44d4-b1b2-eb58fba73482",
   "metadata": {},
   "source": [
    "  12. Which model --- linear model or classification and regression tree --- has better performance on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c23538-a89d-4133-979f-80c134cb488b",
   "metadata": {},
   "source": [
    "> The best linear model with degree=2 achieved an Rsq/RMSE of .74/.196 while the best tree with a max_depth=9 achieved an Rsq/RMSE of .61/.24. In this case, it appears the linear model has better performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5562d29-fcf6-4141-9f5b-a60e25a93477",
   "metadata": {},
   "source": [
    "**Q2.** The is a question about regression using decision trees and linear models. The data include wages at data science-y jobs, including\n",
    "\n",
    "  - `Rating`: Company worker happiness score\n",
    "  - `Size`: Number of employees\n",
    "  - `Sector`: Part of the economy\n",
    "  - `avg_salary`: Average wage\n",
    "  - `job_state`: Location of work\n",
    "\n",
    "  1. Load the `wages_hw.csv` file. Split the sample into an ~80% training set and a ~20% test set. Do any necessary cleaning, including outliers and missings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d19463-883b-47ec-a2a2-5ccc00e389f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./data/wages_hw.csv')\n",
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "df['Intercept'] = np.ones(df.shape[0])\n",
    "print( np.sum(df.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073a892-d343-4c70-9a3a-f3377a1db923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sector'] = df['Sector'].str.replace('-1','Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71e9c0-241e-402f-a61c-7615874bd70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f06cb-f60a-4491-8ec4-d96e38e246af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c643a6-d898-4718-92b0-a8f9c251c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df986b-c90b-4509-86c7-1a6a640371c4",
   "metadata": {},
   "source": [
    "> It looks like some salaries are outliers, but I am going to go with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35445a-f31d-41bf-a7a8-19a66e6966b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = df['Sector'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ca9ec-e690-4664-9c5f-08908738d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "N = df.shape[0]\n",
    "train_size = int(.8*N)\n",
    "\n",
    "index = np.arange(N)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "train_rows = index[:train_size]\n",
    "test_rows = index[train_size:]\n",
    "\n",
    "y = df['avg_salary']\n",
    "y_train = y.iloc[train_rows]\n",
    "y_test = y.iloc[test_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8feac8f-16cd-4012-88e2-9738d3e97177",
   "metadata": {},
   "source": [
    "  2. Use a linear model to regress `avg_salary` on `Sector`. Which sectors have the highest predicted wages? What is the $R^2$ and `RMSE` on the test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210c69d-63d1-4ae4-b365-a32b3fad5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "dummies = pd.get_dummies( df.loc[:,'Sector'], dtype=int)\n",
    "X_train = dummies.iloc[train_rows,:]\n",
    "X_test = dummies.iloc[test_rows,:]\n",
    "#\n",
    "reg = linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train) # Run regression\n",
    "\n",
    "# Extract wage dummies\n",
    "rdf = pd.DataFrame({'variable': reg.feature_names_in_, 'slope':reg.coef_})\n",
    "rdf.sort_values(by='slope',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a901596-86c8-4a67-91ef-3ea748efb3a3",
   "metadata": {},
   "source": [
    "> Media, Accounting and Legal, Information Technology, and Biotech and Pharma have the highest values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e51a9-f503-4d8b-9406-402ee7090459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rsq and RMSE\n",
    "y_hat = reg.predict(X_test)\n",
    "print('Rsq: ', reg.score(X_test,y_test)) # R2\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ) )\n",
    "print('RMSE: ', rmse) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcba6fc-da15-4294-9e17-a7b9f4751e7e",
   "metadata": {},
   "source": [
    "> The Rsq is not so bad! At least it is non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9d6c9-b4f2-4941-80ed-34d42673ca07",
   "metadata": {},
   "source": [
    "  3. Make a scatterplot of `avg_salary` and `Rating`. Is there an obvious visual relationship between the two variables? Regress `avg_salary` on `Rating` as a numeric variable: Do higher ratings predict higher or lower wages? Convert `Rating` to a one-hot encoded variable, with a category for each rating. Run a regression of `avg_salary` on the categorical version. How do your results change? Explain. Which version has a higher $R^2$ and lower `RMSE`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33d605-8573-4b01-9bb0-bc659c5460f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,x='Rating',y='avg_salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d29e7a-6d1c-47f0-90b9-ff3a87071470",
   "metadata": {},
   "source": [
    "> Looks like a cloud of points to me, with no particular relationship. I will not be surprised if the Rsq is not strictly positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41cdef8-1386-4ffe-9657-06e8742a01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X_train = df.loc[train_rows,['Intercept','Rating'] ]\n",
    "X_test = df.loc[test_rows,['Intercept','Rating'] ]\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train) # Run regression\n",
    "\n",
    "# Rsq and RMSE\n",
    "X_test = df.loc[test_rows, ['Intercept','Rating'] ]\n",
    "print('Rsq: ', reg.score(X_test,y_test)) # R2\n",
    "y_hat = reg.predict(X_test)\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "print('RMSE: ', rmse) \n",
    "\n",
    "fx_num = reg.coef_[0] + reg.coef_[1]*np.arange(2,6,1)\n",
    "gdf1 = pd.DataFrame({'Rating':np.arange(2,6,1),'Effect':fx_num,'Version':'Numeric'})\n",
    "\n",
    "rdf = pd.DataFrame({'variable': reg.feature_names_in_, 'slope':reg.coef_})\n",
    "rdf.sort_values(by='slope',axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b576fd-593b-4369-8996-e1553f6e924d",
   "metadata": {},
   "source": [
    "> So if rating goes up by 1, predicted wages go up by 9.5k, with a starting salary for \"zero star\" companies of 65k. That Rsq is not so great, but we didn't see a strong relationship between rating and avg_salary in the scatterplot anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4649a2f-e184-481d-bd9a-e02973b444ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's one-hot encode the ratings\n",
    "df['Rating_rnd'] = df['Rating'].round()\n",
    "\n",
    "X_train = df.loc[train_rows,['Rating_rnd'] ]\n",
    "dummies_train = pd.get_dummies(X_train['Rating_rnd'],dtype=int)\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=False).fit(dummies_train,y_train) # Run regression\n",
    "\n",
    "# Rsq and RMSE\n",
    "X_test = df.loc[test_rows,['Rating_rnd'] ]\n",
    "dummies_test = pd.get_dummies(X_test['Rating_rnd'],dtype=int)\n",
    "y_hat = reg.predict(dummies_test)\n",
    "\n",
    "print('Rsq: ', reg.score(dummies_test,y_test)) # R2\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "print('RMSE: ', rmse) \n",
    "\n",
    "rdf = pd.DataFrame({'variable': dummies_train.columns, 'slope':reg.coef_})\n",
    "rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ca8f6-a9cd-4294-8f83-774025dee708",
   "metadata": {},
   "source": [
    "> What's the real difference between the numeric regression and the dummy regression? You can see in the next plot, the numeric regression forces all the points onto a line with the same slope, while the dummy version allows the \"step sizes\" between ratings to vary.\n",
    "> That Rsq is not great! It is less than zero, which means that using the mean of the training data as a predictor rather than the dummies does a better job on the test set. It's not the end of the world, don't do more work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b330a-0f96-4245-a5a9-ff34a635afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2 = pd.DataFrame({'Rating':np.arange(2,6,1),'Effect':reg.coef_,'Version':'Categorical'})\n",
    "gdf =  pd.concat([gdf1,gdf2],axis=0)\n",
    "sns.lineplot(data=gdf,x='Rating',y='Effect',hue='Version')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a631a-a058-4ba5-8a71-c1d0dfcbea8a",
   "metadata": {},
   "source": [
    "> So going from 4 star to 5 star seems to possibly flatten or reduce wages. Why might that be? One possible explanation: If you're at a place where it's actually nice to work, that's part of your compensation and they don't have to pay as much to get you to work there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73631a8-8940-40e1-af97-439cecbe50f9",
   "metadata": {},
   "source": [
    "  4. Now interact `Sector` with the categorical version of `Rating`, so your regressors are a (Sector, Rating) pair; this is a programming puzzle you'll have to think about, but using the `.PolynomialFeatures()` function on the one-hot encoded categorical variables is one option, and another is pre-processing a new variable that interacts `Sector` and `Rating` and then one-hot encoding the result. Regress `avg_salary` on the (Sector, Rating) pairs. How does the $R^2$ and `RMSE` on the test set compare to part 2? Interpret the coefficients; which sector-rating pairs have the highest wages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2e05a-f8b7-4eac-8abf-3cd1cac8d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SxR'] = df.loc[:,'Sector'] + 'X' + df.loc[:,'Rating_rnd'].astype(str)\n",
    "dummies = pd.get_dummies(df.loc[:,'SxR'], dtype=int ) \n",
    "\n",
    "# Fit linear model\n",
    "dummies_train = dummies.iloc[train_rows,:]\n",
    "reg = linear_model.LinearRegression(fit_intercept=False).fit(dummies_train,y_train) # Run regression\n",
    "rdf = pd.DataFrame({'variable': dummies_train.columns, 'slope':reg.coef_})\n",
    "print(rdf.sort_values(by='slope',axis=0).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7224dd-14cf-4f15-bcb9-a27950e40e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_test = dummies.iloc[test_rows,:]\n",
    "y_hat = reg.predict(dummies_test)\n",
    "print('Rsq: ', reg.score(dummies_test,y_test)) # R2\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "print('RMSE: ', rmse) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30723fc-a831-4f94-9ced-021e0f1e909c",
   "metadata": {},
   "source": [
    "  5. Run a linear regression of `avg_salary` on all the variables. What is the $R^2$ on the test set? How does it compare to your simpler models in 2--4? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9c28d-b08e-4e1a-b6ba-01728ce6cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dummies = pd.get_dummies(df['job_state'],dtype=int)\n",
    "size_dummies = pd.get_dummies(df['Size'],dtype=int)\n",
    "sector_dummies = pd.get_dummies(df['Sector'],dtype=int)\n",
    "\n",
    "X_num = df.loc[:,['Intercept','Rating_rnd'] ] \n",
    "\n",
    "X = pd.concat([X_num, state_dummies, size_dummies, sector_dummies],axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45470192-68bb-49b2-aa0b-836cccdffe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X_train = X.iloc[train_rows,:]\n",
    "X_test = X.loc[test_rows,:]\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train) # Run regression\n",
    "\n",
    "# Rsq and RMSE\n",
    "print('Rsq: ', reg.score(X_test,y_test)) # R2\n",
    "y_hat = reg.predict(X_test)\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "print('RMSE: ', rmse) \n",
    "\n",
    "rdf = pd.DataFrame({'variable': reg.feature_names_in_, 'slope':reg.coef_})\n",
    "rdf.sort_values(by='slope',axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4593990-e633-4ac5-beb1-186c40b6ddee",
   "metadata": {},
   "source": [
    "> That's quite a negative Rsq! This model is overfit. Our best model so far is just the simple regression on sector. So it's nice we have all these variables, but including all of them is leading to multicolinearity or bad control, negative Rsq, and huge RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04893788-23f3-42c2-908b-5e4c5b730821",
   "metadata": {},
   "source": [
    "  6. Build a decision tree by regressing `avg_salary` on `Sector`, `Rating`, and the (Sector, Rating) pairs. What are the $R^2$ and `RMSE` of your models on the test set? How do your answers compare to parts 2, 3, and 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f687e-47bd-49ea-a6f4-cb0f2dcaa434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "\n",
    "# 2.\n",
    "dummies = pd.get_dummies( df.loc[:,'Sector'], dtype=int)\n",
    "X_train = dummies.iloc[train_rows,:]\n",
    "X_test = dummies.iloc[test_rows,:]\n",
    "\n",
    "model = tree.DecisionTreeRegressor()\n",
    "cart = model.fit(X_train, y_train) # \n",
    "\n",
    "y_hat = cart.predict(X_test)\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "\n",
    "print('Part 2, just sector. Rsq: ', cart.score(X_test,y_test), ', RMSE: ', rmse)\n",
    "\n",
    "\n",
    "# 3.\n",
    "X_train = df.loc[train_rows,['Intercept','Rating'] ]\n",
    "X_test = df.loc[test_rows,['Intercept','Rating'] ]\n",
    "\n",
    "model = tree.DecisionTreeRegressor()\n",
    "cart = model.fit(X_train, y_train) # \n",
    "\n",
    "y_hat = cart.predict(X_test)\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "\n",
    "print('Part 3, just rating. Rsq: ', cart.score(X_test,y_test), ', RMSE: ', rmse)\n",
    "\n",
    "\n",
    "# 4.\n",
    "df['SxR'] = df.loc[:,'Sector'] + 'X' + df.loc[:,'Rating_rnd'].astype(str)\n",
    "dummies = pd.get_dummies(df.loc[:,'SxR'], dtype=int ) \n",
    "X_train = dummies.iloc[train_rows,:]\n",
    "X_test = dummies.iloc[test_rows,:]\n",
    "\n",
    "model = tree.DecisionTreeRegressor()\n",
    "cart = model.fit(X_train, y_train) # \n",
    "\n",
    "y_hat = cart.predict(X_test)\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "\n",
    "print('Part 4, sector-rating. Rsq: ', cart.score(X_test,y_test), ', RMSE: ', rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b8a53-b15e-470f-987a-4b9479b3568a",
   "metadata": {},
   "source": [
    "> For parts 2 and 3, the performance is the same. Why does the decision tree do much better in part 4? Let's take a dangerous step and actually look at the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb81423-14a6-424d-aead-b6b1f4be406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(cart,filled=True,feature_names = X_train.columns.tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166a9ac-47c1-4ba4-8712-62c29902d3c1",
   "metadata": {},
   "source": [
    "> So the answer goes something like this: The linear regression is committed to including the variables you pass to `.LinearRegression`. It can't decide that some variables are good and others are bad. On the other hand, the decision tree is going to make those decisions itself. So it essentially \"drops\" a huge number of the SectorXRating interactions, giving it more freedom to make data driven decisions about what kinds of interactions matter and which are just noise. Ultimately, you get a respectable Rsq of .235, much better than the linear model's best performance of .107. Now, there is a tool we'll cover later called the LASSO which does a similar thing for linear regression, maintaining interpretablility but also dropping the variables that are increasing predictor variance without reducing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95f13a-1d87-4906-8027-4f86d9386f75",
   "metadata": {},
   "source": [
    "  7. Build a decision tree by regressing `avg_salary` on all the other variables. What is the $R^2$ and `RMSE` on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94c73e-3818-424c-be3f-e2a40ee77bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Variables:\n",
    "state_dummies = pd.get_dummies(df['job_state'],dtype=int)\n",
    "size_dummies = pd.get_dummies(df['Size'],dtype=int)\n",
    "sector_dummies = pd.get_dummies(df['Sector'],dtype=int)\n",
    "\n",
    "# Numeric Variables:\n",
    "X_num = df.loc[:,['Intercept','Rating_rnd'] ] \n",
    "X = pd.concat([X_num, state_dummies, size_dummies, sector_dummies],axis=1)\n",
    "\n",
    "# Train-test split:\n",
    "X_train = X.iloc[train_rows,:]\n",
    "X_test = X.loc[test_rows,:]\n",
    "\n",
    "model = tree.DecisionTreeRegressor()\n",
    "cart = model.fit(X_train, y_train) # \n",
    "\n",
    "y_hat = cart.predict(X_test)\n",
    "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
    "\n",
    "print('Part 4, sector-rating. Rsq: ', cart.score(X_test,y_test), ', RMSE: ', rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043766a-5f74-48bf-9f54-8bb8e17c2cc8",
   "metadata": {},
   "source": [
    "  8. Build a linear regression or decision tree using the available variables based on your own judgment. What degrees of freedom are you giving the model to predict variation in wages across company and location attributes? What is the $R^2$ and `RMSE` of your model? How does it compare to the previous ones in the question? Why does yours perform better or worse on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5863c-5af5-4dcb-82b2-0d18bdfea9a9",
   "metadata": {},
   "source": [
    "> Depends on the student."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fda7dc-6b1b-4a1a-8d1e-cb8cb061d2ae",
   "metadata": {},
   "source": [
    "**Q3.** This a question purely on categorical prediction. The data for this happen to be gathered in 1987 in Indonesia, and concern contraceptive method choice. The questions and data-gathering assumptions reflect the culture and attitudes of that time and place, but provide a good example of a categorical prediction problem on an important topic (family planning and maternal health The variables in the data are:\n",
    "\n",
    "    - Wife's age (numerical)\n",
    "    - Wife's education (categorical) 1=low, 2, 3, 4=high \n",
    "    - Husband's education (categorical) 1=low, 2, 3, 4=high \n",
    "    - Number of children ever born (numerical) \n",
    "    - Wife's religion (binary) 0=Non-Islam, 1=Islam\n",
    "    - Wife's now working? (binary) 0=Yes, 1=No\n",
    "    - Husband's occupation (categorical) 1, 2, 3, 4\n",
    "    - Standard-of-living index (categorical) 1=low, 2, 3, 4=high\n",
    "    - Media exposure (binary) 0=Good, 1=Not good\n",
    "    - Contraceptive method used (class attribute) 1=No-use, 2=Long-term, 3=Short-term\n",
    "\n",
    "  1. Load the `contraceptiveMethodChoice.csv` data. Tabulate the `method` variable (i.e. `.value_counts()`). 1 corresponds to `No Contraception`, 3 corresponds to `Short Term` (e.g. condoms, birth control pills), and 2 corresponds to `Long Term` (e.g. IUD, sterilization). Cross tabulate `method` and `numberChildren`. Do couples that use birth control tend to have more children than those who don't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c31fcf-1845-4fe3-8269-fb77e8e7a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./data/contraception_hw.csv')\n",
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "print( np.sum(df.isnull()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6f188-0d78-4374-8537-788e65a6cb1a",
   "metadata": {},
   "source": [
    "  2. Split the sample into ~80% training data and ~20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad757d08-6869-4c45-aac1-1c5e0d970c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('method',axis=1), df['method'], test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445d70c-da60-4df4-b868-01ffd4e83176",
   "metadata": {},
   "source": [
    "  3. We are now going to make a mistake. Train a regression tree to predict the contraceptive method using the other variables in the data, not a classification tree. Look at the terminal nodes in the tree: What values do they take? Does that make sense? Explain clearly what has gone wrong here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87722c24-56a1-46aa-b8f9-36eecd263162",
   "metadata": {},
   "source": [
    "> I restricted the `max_depth` parameter to 2 so I could make sense of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc180670-eae6-40c8-8cad-bc514800d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "model = tree.DecisionTreeRegressor(max_depth=2, random_state=100) # Fit the classifier\n",
    "cart = model.fit(X_train, y_train)\n",
    "tree.plot_tree(cart,filled=True,feature_names = X_train.columns.tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2deb5f-a3a1-442a-b8b4-96ba3f9ef38a",
   "metadata": {},
   "source": [
    "> At the bottom of the tree, we have values like 1.026 or 2.112. These aren't probabilities of using one of the three methods, and they're not methods themselves. This is just bad prediction. We want to predict 1, 2 or 3, not an intermediate value between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430ae49-63c9-45fb-870a-089933d1d575",
   "metadata": {},
   "source": [
    "  4. Instead of regression, use a classification tree to predict contraceptive method using the other variables in the data. How does it look different from the previous tree? What variables does the algorithm use? In broad terms, which groups of people are most likely to use each method of contraception?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e544ac-0e79-43d1-87ab-2972cf272367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth=2,random_state=100) # Fit the classifier\n",
    "cart = model.fit(X_train, y_train)\n",
    "tree.plot_tree(cart,filled=True,feature_names = X_train.columns.tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5288671-0439-4bd8-87b3-73657dae8627",
   "metadata": {},
   "source": [
    "> Let's recall, contraceptive use is 1=No-use, 2=Long-term, 3=Short-term. For respondents under 18 with no children, most are using no contraception or short-term contraception. For respondents with a child, respondents with less than the highest recorded education level mostly are not using contraception (324/676) or short-term contraception (252/676), while those with the highest education level are split equally across the three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015af52d-7602-47ed-8eeb-67e50fff01da",
   "metadata": {},
   "source": [
    "  5. Compute a confusion matrix for your classification tree on the test set (Hint: There are now three categories instead of two, so the cross tabulation will be a $3 \\times 3$ matrix instead of $2 \\times 2$.). Compute the Accuracy of your model overall, and the Accuracy for predicting each contraceptive method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b00240-c163-47b3-aa9f-1192f36d8fd8",
   "metadata": {},
   "source": [
    "> To give the model more of a chance to do a good job at predicting, I'll remove the max_depth constraint and just let it run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d96c82-e219-4bfc-a670-724d82980874",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(random_state=0) # Fit the classifier\n",
    "cart = model.fit(X_train, y_train)\n",
    "y_hat = cart.predict(X_test)\n",
    "\n",
    "tab = pd.crosstab(y_hat, y_test)\n",
    "acc = (tab.iloc[0,0]+tab.iloc[1,1]+tab.iloc[2,2])/len(y_test)\n",
    "print('Accuracy: ',acc)\n",
    "\n",
    "acc1 = tab.iloc[0,0]/(tab.iloc[0,0]+tab.iloc[0,1]+tab.iloc[0,2])\n",
    "print('Accuracy, 1: ',acc1)\n",
    "\n",
    "acc2 = tab.iloc[1,1]/(tab.iloc[1,0]+tab.iloc[1,1]+tab.iloc[1,2])\n",
    "print('Accuracy, 2: ',acc2)\n",
    "\n",
    "acc3 = tab.iloc[2,2]/(tab.iloc[2,0]+tab.iloc[2,1]+tab.iloc[2,2])\n",
    "print('Accuracy, 3: ',acc3)\n",
    "\n",
    "pd.crosstab(y_hat, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565dc2b4-cd7a-476a-9723-746f8bc628e3",
   "metadata": {},
   "source": [
    "> Overall, the accuracy is .508. If you think about whether it does a better job predicting each method individually, we can compute the number correct divided by number predicted to get an accuracy for each prediction. Accuracy is best for no contraception at .589, followed by short-term at .469, followed by long-term at .425. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91093f5-ae54-4d26-81aa-8267105de3b7",
   "metadata": {},
   "source": [
    "  7. Why can't you use a linear probability model to do this exercise? Explain clearly in words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5d223-386b-4c66-a3a1-9ae21105e657",
   "metadata": {},
   "source": [
    "> An LPM $ y = b\\cdot x$ splits the space into two groups: Those for whom $b \\cdot x >0$ and those for whom $b \\cdot x \\le 0$. This only allows you to distinguish between two categories, not three or more. There are plenty of methods that use a linear model like $b \\cdot x$ as a workhorse component of a non-linear model which allows more complex decision rules, like multinomial logit or support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8050c6f-762d-452f-885f-c4676f2b04aa",
   "metadata": {},
   "source": [
    "**Q4.** This is a question where we use regression and regression trees. The outcome is whether a defendant is held pre-trial in the Virginia justice system. We would like to understand how that outcome is predicted by characteristics of the defendant, particularly race. Let's be very careful/clear: We aren't saying anyone *should* be held without bond or asserting that people with different demographic variables *should* be more likely to be held, but instead trying to predict whether people with different characteristics *are empirically more likely* to be held without bond, given the available information. This is the first step we would take in investigating whether a system is fair, or how large the disparities are: Does it treat people with similar observable characteristics similarly, or not? We are going to look at a common question: Are Black defendants treated differently from white or Asian ones? (There are Native American defendants, but there are 11 in total, which is such a small number of observations that is difficult to clearly say anything about how this group is treated relative to the others.)\n",
    "\n",
    "The variables in the data are:\n",
    "\n",
    "  - `held_wo_bail`: Whether a defendant is held without bail before trial (Boolean logical)\n",
    "  - `race`, `sex`: Categorical demographic variables\n",
    "  - `is_poor`: Whether the defendant is classified as indigent \n",
    "  - `prior_F`, `prior_M`: The number of prior felony and misdemeanor arrests\n",
    "  - `case_type`: A categorical variable indicating a misdemeanor `M` or felony `F` or infraction `I` or special case `S`\n",
    "  - `age`: Defendant's age\n",
    "  - `bond`, `bond_NA`, `bond_type`: The amount of any bond, whether it is missing, and the type\n",
    "  - `sentence`, `sentence_NA`, `sentence_type`: The length of any sentence, whether it is missing, and the type\n",
    "\n",
    "1. Load the `pretrial_data.csv` data. Notice that there are `nan`s, but the data are relatively clean. Because there are `.nan`s among variables you won't use, you'll want to narrow down your analysis to the relevant variables before dropping or imputing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f4e78-46b2-4cef-85f5-9785335d0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/pretrial_data.csv')\n",
    "print(df.head())\n",
    "\n",
    "df['held'] = df['held_wo_bail'].astype(int)\n",
    "df['sex'] = df['sex'].replace(['M','F'],['Male','Female'])\n",
    "df['Black'] = (df['race']=='B').astype(int)\n",
    "rdf = df.loc[:,['held','Black','sex','case_type','is_poor','prior_F']]\n",
    "rdf = rdf.dropna()\n",
    "rdf['held'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2671a7-a8e1-46d7-9a7a-2f6e245ea49d",
   "metadata": {},
   "source": [
    "2. Create a dummy variable indicating that the defendant is Black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ec6d7-eab0-4e1f-b1d8-9845d78b21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Black'] = (df['race']=='B').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84783b03-dd95-4763-ac92-cd78d1c5e5c2",
   "metadata": {},
   "source": [
    "3. Regress `held` on `Black`. What is the slope coefficient Interpret the coefficient on the Black dummy variable: How much more likely is a black person to be held without bail? What is the $R^2$ of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f28317-b6b4-45d4-b80e-63a8ccd9904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # Import linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4977d25-3b9f-492e-8fed-2c2b3ff98e0f",
   "metadata": {},
   "source": [
    "4. Before doing this question, please think for a few minutes about how to make the process of running these regressions as efficient as possible, before jumping into writing code. Repeat part 3, for the following specifications, keeping track of the coefficient on the Black dummy variable each time:\n",
    "      - `held` on `Black` and `sex`\n",
    "      - `held` on `Black` and `sex` and `is_poor`\n",
    "      - `held` on `Black` and `sex` and `is_poor` and `prior_F`\n",
    "      - `held` on `Black` and `sex` and `is_poor` and `prior_F` and `case_type`\n",
    "What happens to the coefficient on the Black dummy variable as you include more regressors/features/controls in the regression? Explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87dfad8-2c56-468f-b896-06a97805b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # Import linear regression model\n",
    "\n",
    "sex_dummies = pd.get_dummies(rdf['sex'],dtype='int',drop_first=True)\n",
    "case_type_dummies = pd.get_dummies(rdf['case_type'],dtype='int',drop_first=True)\n",
    "\n",
    "## Black and Sex\n",
    "y = rdf['held']\n",
    "X = pd.concat([rdf['Black'],sex_dummies],axis=1)\n",
    "reg = LinearRegression().fit(X,y)\n",
    "sdf = pd.DataFrame({'variable': X.columns, 'slope':reg.coef_})\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f7c0b-7a61-4415-b02d-fa0f2f34433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Black and Sex and is_poor\n",
    "y = rdf['held']\n",
    "X = pd.concat([rdf.loc[:,['Black','is_poor']],sex_dummies],axis=1)\n",
    "reg = LinearRegression().fit(X,y)\n",
    "sdf = pd.DataFrame({'variable': X.columns, 'slope':reg.coef_})\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cb8fe-46fc-46b1-ac5b-2fcfea426ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Black and Sex and is_poor and prior_F\n",
    "y = rdf['held']\n",
    "X = pd.concat([rdf.loc[:,['Black','is_poor','prior_F']],sex_dummies],axis=1)\n",
    "reg = LinearRegression().fit(X,y)\n",
    "sdf = pd.DataFrame({'variable': X.columns, 'slope':reg.coef_})\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b8cfd-aa39-405d-b1ac-49c9cc33df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Black and Sex and is_poor and prior_F and case_type\n",
    "y = rdf['held']\n",
    "X = pd.concat([rdf.loc[:,['Black','is_poor','prior_F']],sex_dummies,case_type_dummies],axis=1)\n",
    "reg = LinearRegression().fit(X,y)\n",
    "sdf = pd.DataFrame({'variable': X.columns, 'slope':reg.coef_})\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bf99b-edd8-418d-b719-9335bc8fc4b1",
   "metadata": {},
   "source": [
    "5. Suppose we don't want to see just `Black` and `sex`, but `Black` interacted with `sex`: Are Black men and Black women treated systemically differently from the rest of the population? Implement this in a regression, and explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd908dc-f842-41c1-bcbc-69a32a576180",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b10db-9a1e-43f2-ad74-c07080e077c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf['raceXsex'] = rdf['Black'].replace([0,1],['Other','Black'])+'x'+rdf['sex']\n",
    "rdf['raceXsex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7035e11-3ec8-46ba-bea2-72744cf0f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rdf['held']\n",
    "X = pd.get_dummies(rdf['raceXsex'],dtype=int)\n",
    "reg = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "sdf = pd.DataFrame({'variable': X.columns, 'slope':reg.coef_})\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e02f1-2789-4266-900e-4f58ee5ca8ec",
   "metadata": {},
   "source": [
    "> What's interesting here is that Black women have a base probability of .17 of being held without bail, which is the lowest. On the other hand, Black men have a predicted probability of .35 of being held without bail, more than double. Other races have, on average, probabilities in the middle, of .2 for women and .25 for men. One thing to notice is that all of these numbers are surprisingly high: 1 in 10 or 1 in 3 people are held without bail before their trial? This does not seem consistent with \"innocent until proven guilty.\" I think this is interesting but not entirely unexpected, but I would want to look into the data more before making guesses about what's driving the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5266313-3d68-491f-a144-3f2c08cd8e28",
   "metadata": {},
   "source": [
    "6. Imagine someone argued we should use these kinds of models to help a judge or magistrate make bail decisions (you could obviously go back and make this kind of model for the bond and sentence variables, then deploy it on new cases to predict what their bond and sentence values would be). What concerns would you have? Do you think society should be using data-driven and automated tools like that? Explain your concerns clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39e0f6-9795-4959-9451-88640a8e9a13",
   "metadata": {},
   "source": [
    "> Before getting into this, I just want to reiterate that I ask these questions because I care about how data are used and how the tools impact vulnerable people in society. This situation is statistically and computationally more complex than you probably think. I think the legal system discriminates against, among other groups, Black people and in particular Black men. \n",
    "> \n",
    "> Before opposing this idea, let's make a pro argument: \"In cases where the system is extremely biased, it might help to have a quantiative tool that can be explored and subjected to rigorous quantitative examination. Even if that tool is unfair, it might be more fair than some of the systems people actually face.\" In principle, this might be a meaningful step towards improving society, and being optimistic about technology is a totally fair position as long as it is tempered by healthy skepticism.\n",
    ">\n",
    "> The concern is that this kind of tool just uses data from past cases to predict how new cases might be decided. If the past system is not fair or just, predicting how it would behave in the future does not somehow magically create fairness or justice. If we're worried about the old system, working hard to create a predictive algorithm that replicates its choices in order to decide future cases is misguided. \n",
    ">\n",
    "> \"But,\" you might say, \"of course we shouldn't include race in the regression. That's obviously racist. Let's instead drop some variables. Shouldn't that do better?\" The key thing to recognize is that if you drop the Black dummy variable, it doesn't make the situation \"fair\": Just as the coefficient on Black declines in part 4 as relevant covariates are added to the regression, dropping the Black dummy will shift weight onto those covariates in ways that disproportionately predict Black defendants will be held (i.e. omitted variables bias). This will systematically disadvantage Black defendants, regardless of whether you include race in the regression. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d1f6e-8454-42ae-be5c-8b8b9f37b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Black and Sex and is_poor and prior_F\n",
    "y = rdf['held']\n",
    "X = pd.concat([rdf.loc[:,['Black','is_poor','prior_F']],sex_dummies],axis=1)\n",
    "reg = LinearRegression().fit(X,y)\n",
    "sdf = pd.DataFrame({'variable': X.columns, 'slope':reg.coef_})\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba46e36-c9d6-4eda-b84c-50c21a17da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Black and Sex and is_poor and prior_F\n",
    "y = rdf['held']\n",
    "X = pd.concat([rdf.loc[:,['is_poor','prior_F']],sex_dummies],axis=1)\n",
    "reg = LinearRegression().fit(X,y)\n",
    "sdf = pd.DataFrame({'variable': X.columns, 'slope':reg.coef_})\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5de931-d841-46ee-9f9f-3eb0402ebc64",
   "metadata": {},
   "source": [
    "> When we drop the Black dummy, the coefficients on the other variables increase. Why? Among the Black defendants in these data, they happen to have higher rates of indigency, prior felony convictions, and likelihood of being Male. Black defendants will still likely get worse outcomes, even if you try to hide information about them from the algorithm. \n",
    ">\n",
    "> There is not a simple statistical procedure to address these issues. People should be very skeptical that raw data manipulation and machine learning can replace human decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21917e39-ef41-4799-abfd-b99e672747c5",
   "metadata": {},
   "source": [
    " **Q5.** This is a math question to review the derivation of the OLS estimator (but only if you are into that kind of thing!). We are going to do it slightly differently from what we did in class, though. We will use a linear predictor and minimize the Sum of Squared Errors, just as in class. But, we are going to de-mean $X$ first, creating another variable $z_i = x_i - \\bar{x}$ where \n",
    "$$\n",
    "\\bar{x} = \\dfrac{1}{N} \\sum_{i=1}^N x_i,\n",
    "$$\n",
    "so the model is $\\hat{y}_i = a + b z_i$ and the `SSE` is\n",
    "$$\n",
    "\\text{SSE}(a,b) = \\sum_{i=1}^N (y_i - a - bz_i)^2.\n",
    "$$\n",
    "\n",
    "  1. Take partial derivatives of the `SSE` with respect to $a$ and $b$. You should get\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\sum_{i=1}^N -2(y_i - a- bz_i) &=& 0 \\\\\n",
    "\\sum_{i=1}^N -2(y_i - a - bz_i)z_i &=& 0.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "> Take the 2 down when you differentiate and decrement the exponent, and use the chain rule to get the -1 and $-z_i$.\n",
    "\n",
    "  2. Solve for the solutions to the above equations. Big hint: $\\bar{z} = 0$, since we subtracted the mean of $x$ from $x$ to get $z$. You should get\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "a^* &=& \\bar{y} \\\\\n",
    "b^* &=& \\dfrac{\\sum_{i=1}^N(y_i - \\bar{y})z_i}{\\sum_{i=1}^N z_i^2}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "> More details for equation 1:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "0 &=& \\sum_{i=1}^N -2(y_i - a- bz_i) \\\\\n",
    "&=& \\sum_{i=1}^N y_i - a- bz_i \\\\\n",
    "&=& \\sum_{i=1}^N y_i - N a- b \\sum_{i=1}^Nz_i \\\\\n",
    "&=& \\dfrac{\\sum_{i=1}^N y_i}{N} -  a- b \\dfrac{\\sum_{i=1}^N z_i}{N} \\\\\n",
    "&=& \\bar{y} -  a - b \\bar{z} \\\\\n",
    "&=& \\bar{y}-  a - b 0 \\\\\n",
    "a &=& \\bar{y}\n",
    "\\end{eqnarray*}\n",
    "Notice where $\\bar{z}=0$ gets used.\n",
    "\n",
    "> More details for equation 2:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "0 &=& \\sum_{i=1}^N -2(y_i - a - bz_i)z_i \\\\\n",
    "&=& \\sum_{i=1}^N -2(y_i - \\bar{y} - bz_i)z_i \\\\\n",
    "&=& \\sum_{i=1}^N (y_i - \\bar{y})z_i - b\\sum_{i=1}^N z_i^2 \\\\\n",
    "b\\sum_{i=1}^N z_i^2 &=& \\sum_{i=1}^N (y_i - \\bar{y})z_i \\\\\n",
    "b &=& \\dfrac{\\sum_{i=1}^N (y_i - \\bar{y})z_i}{\\sum_{i=1}^N z_i^2} \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "  3. Substitute $z_i = x_i - \\bar{x}$ back into the above equations. You should get\n",
    "  \n",
    "\\begin{eqnarray*}\n",
    "a^* &=& \\bar{y} \\\\\n",
    "b^* &=& \\dfrac{\\sum_{i=1}^N(y_i - \\bar{y})(x_i-\\bar{x})}{\\sum_{i=1}^N (x_i-\\bar{x})^2},\n",
    "\\end{eqnarray*}\n",
    "\n",
    "which can be written in terms of sample covariance and sample variance as:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "a^* &=& \\bar{y} \\\\\n",
    "b^* &=& \\dfrac{\\text{cov}(x,y)}{\\text{var}(x)}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This is typically the preferred way of expressing the OLS coefficients.\n",
    "\n",
    "> Indeed.\n",
    "\n",
    "4. When will $b^*$ be large or small, depending on the relationship between $x$ and $y$ and the amount of \"noise\"/variance in $x$? What does $a^*$ represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898fca9a-6d6c-41ff-ae17-fe10501f48a7",
   "metadata": {},
   "source": [
    "> If the covariance of $x$ and $y$ is small or the variance of $x$ is large, the slope coefficient will be small. The intercept is just the average value of $y$. These are useful rules-of-thumb to keep in mind about what slopes and intercepts are mechanically in regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4ea8c-2f17-4781-8ecf-c64f60ff5b59",
   "metadata": {},
   "source": [
    "5. Suppose you have measurement error in $x$ which artificially inflates its variance (e.g. bad data cleaning). What happens to the $b^*$ coefficient? How will affect your ability to predict? (This phenomenon is called **attenuation**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702fcd0-2ab1-44c0-b03b-81a9608dec0b",
   "metadata": {},
   "source": [
    "> Measurement error will make the variance of $x$ go up, and cause the coefficient to shrink. This means your predictions will be biased towards 0, since the coefficient is shrunk from its true value. Your estimates will be much more conservative than they would be if your data were clean. Clean your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0064e9-a806-4bdb-b67e-3776ce553bed",
   "metadata": {},
   "source": [
    "6. Let's return to the question of *outliers*. With your formula for the OLS coefficients $(a^*,b^*)$, explain what happens if you significantly increase a single value of the outcome/target/response variable $y_i$ or one of the predictor/explanatory/covariate variables $x_i$. If values for some extreme observations are exerting significant influence over the regression coefficients, will the model perform well on for more average observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782afa99-a057-4c19-a253-1d80b7354612",
   "metadata": {},
   "source": [
    "> Consider one pair $(x_i,y_i)$. Changes in $x_i$ don't affect the intercept but do affect the intercept, while changes in $y_i$ shift the intercept directly. For the slope, $$ b^* = \\dfrac{(x_i - \\bar{x})(y_i-\\bar{y})+\\sum_{k \\neq i}(x_k - \\bar{x})(y_k-\\bar{y}) }{ (x_i - \\bar{x})^2 + \\sum_{k \\neq i} (x_k - \\bar{x})^2}.$$ Increasing $y_i$ will increase the slope coefficient in absolute value at a rate of about $(n-1)/n$, since it appears in $\\bar{y}$. Increasing $x_i$ makes the denominator grow faster than the numerator, so it will shrink the slope coefficient as $x_i$ gets very large (i.e., becomes an outlier).\n",
    ">\n",
    "> A more detailed discussion goes like this. Let's take the derivative of $b^*$ with respect to $y_i$. That equals\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b24df-82e8-4d7f-a22f-3200155fbb6e",
   "metadata": {},
   "source": [
    "$$ \\dfrac{db^*}{dy_i} = \\dfrac{(x_i-\\bar{x})(1-1/n) -(1/n) \\sum_{k\\neq i}(x_k-\\bar{x})}{\\text{var}(x)} = \\dfrac{x_i - \\bar{x}}{\\text{var}(x)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5908d9-008d-4d38-8ade-c4188e2845ac",
   "metadata": {},
   "source": [
    "The derivative with respect to $x_i$ is much more complicated because it appears in the numerator and denominator:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eaec5c-5ca9-47aa-8941-36bace85f28b",
   "metadata": {},
   "source": [
    "$$\\dfrac{db^*}{dx_i} = \\dfrac{ \\left((1-1/n)(y_i - \\bar{y}) -(1/n) \\sum_{k \\neq i}(y_k - \\bar{y})\\right) \\text{var}(x) - \\left(2(x_i-\\bar{x})(1-1/n) - (2/n) \\sum_{k \\neq i}2(x_k - \\bar{x})\\right) \\text{cov}(x,y)}{\\text{var}(x)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00027bd-0732-42e9-93d7-54ace40d83b4",
   "metadata": {},
   "source": [
    "> Similar to $db^*/dy_i$, this could probably be simplified further but I'll stop there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
