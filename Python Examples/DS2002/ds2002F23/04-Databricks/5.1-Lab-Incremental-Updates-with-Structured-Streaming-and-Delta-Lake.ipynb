{"cells":[{"cell_type":"markdown","source":["### Processing Incremental Updates with Structured Streaming and Delta Lake\nIn this lab you'll apply your knowledge of structured streaming and Auto Loader to implement a simple multi-hop architecture.\n\n#### 1.0. Import Shared Utilities and Data Files\nRun the following cell to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42af51fd-bef4-47ca-bb96-53e5e488f554"}}},{"cell_type":"code","source":["%run ./Includes/5.1-Lab-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f02d4ec-6520-4906-bd69-9cd120265130"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 2.0. Bronze Table: Ingest data\nThis lab uses a collection of customer-related CSV data from DBFS found in */databricks-datasets/retail-org/customers/*.  Read this data using Auto Loader using its schema inference (use **`customersCheckpointPath`** to store the schema info). Stream the raw data to a Delta table called **`bronze`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"431e528d-711c-4144-9ed2-5875a0de7f24"}}},{"cell_type":"code","source":["# TODO:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"176594fc-08a2-4dde-b02f-095098cabd42"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DA.block_until_stream_is_ready(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e992ca75-1679-4b11-aab8-369b3bc5b7ea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 2.1. Create a Streaming Temporary View\nCreate a streaming temporary view into the bronze table so that we can perform transformations using SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60a19272-7a96-48f1-821a-3f97df06303e"}}},{"cell_type":"code","source":["(spark\n  .readStream\n  .table(\"bronze\")\n  .createOrReplaceTempView(\"bronze_temp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03be6ed8-4bad-46b9-bb8b-bbb6a33c2f7a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 2.2. Clean and Enhance the Data\nUse the CTAS syntax to define a new streaming view called **`bronze_enhanced_temp`** that does the following:\n* Skips records with a null **`postcode`** (set to zero)\n* Inserts a column called **`receipt_time`** containing a current timestamp\n* Inserts a column called **`source_file`** containing the input filename"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65e84cd0-c27e-4eb8-9bb5-d205b0894b17"}}},{"cell_type":"code","source":["%sql\n-- TODO:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcd64fdd-9b5d-45bc-b641-b4162ceacb63"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.0. Silver Table\nStream the data from **`bronze_enhanced_temp`** to a table called **`silver`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b00d377b-c499-4c5d-b72d-f3a992c8fc6f"}}},{"cell_type":"code","source":["# TODO:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac90bbf8-c569-48ed-aa6f-59c6de0ffa42"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DA.block_until_stream_is_ready(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"269439ac-61dd-49ea-ae1a-e7fe22eb14b5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 3.1. Create a Streaming Temporary View\nCreate another streaming temporary view for the silver table so that we can perform business-level queries using SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3db953ec-fbad-49f4-b4bf-2b2e87b7f102"}}},{"cell_type":"code","source":["(spark\n  .readStream\n  .table(\"silver\")\n  .createOrReplaceTempView(\"silver_temp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29fb7ae0-3cc4-4fdc-a39c-436888567478"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 4.0. Gold Table\nUse the CTAS syntax to define a new streaming view called **`customer_count_temp`** that counts customers per state."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f627fdb-0801-466a-b551-cbb2475db2b9"}}},{"cell_type":"code","source":["%sql\n-- TODO:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae488156-8f83-4687-85c9-9ad0634d3070"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, stream the data from the **`customer_count_by_state_temp`** view to a Delta table called **`gold_customer_count_by_state`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d74549f-2c3a-4e8f-b09d-b3f9d884e1c9"}}},{"cell_type":"code","source":["# TODO:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93c5d8a0-882d-4657-a22f-6a52a4ff2f03"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DA.block_until_stream_is_ready(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"853b65f6-afa4-43a8-b014-3d11115b02a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 5.0. Query the Results\nQuery the **`gold_customer_count_by_state`** table (this will not be a streaming query). Plot the results as a bar graph and also using the map plot."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca186f2a-62ff-4b1f-be75-936190dbd058"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM gold_customer_count_by_state"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12a846b2-8a76-4c20-accc-8a350befa08f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 6.0. Clean Up\nRun the following cell to remove the database and all data associated with this lab."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90cbf4de-2269-4a8a-9d23-be8128621149"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2633ccc7-cde5-4fd1-8351-268d39b8d97e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["By completing this lab, you should now feel comfortable:\n* Using PySpark to configure Auto Loader for incremental data ingestion\n* Using Spark SQL to aggregate streaming data\n* Streaming data to a Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27599803-99c9-411d-bb91-1f12d0c8a214"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"5.1-Lab-Incremental-Updates-with-Structured-Streaming-and-Delta-Lake","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1225804990858222}},"nbformat":4,"nbformat_minor":0}
